{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6DE1fhOrnMk"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# =========================================================\n",
        "# 0. Setup\n",
        "# =========================================================\n",
        "!pip -q install pandas scikit-learn torch torchvision transformers datasets evaluate seaborn matplotlib\n",
        "\n",
        "import pandas as pd, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import files\n",
        "from tqdm.auto import tqdm\n",
        "# Import datasets after transformers\n",
        "import datasets\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                          TrainingArguments, Trainer, DataCollatorWithPadding)\n",
        "\n",
        "# =========================================================\n",
        "# 1. Upload & load data\n",
        "# =========================================================\n",
        "uploaded = files.upload()          # choose breastcancer.csv\n",
        "df = pd.read_csv(next(iter(uploaded)))\n",
        "\n",
        "target = \"Cancer Type Detailed\"\n",
        "features = [\n",
        "\"Study ID\",\"Patient ID\",\"Sample ID\",\"Age at Diagnosis\",\"Type of Breast Surgery\",\n",
        "\"Cancer Type\",\"Cellularity\",\"Chemotherapy\",\"Pam50 + Claudin-low subtype\",\"Cohort\",\n",
        "\"ER status measured by IHC\",\"ER Status\",\"Neoplasm Histologic Grade\",\"HER2 status measured by SNP6\",\n",
        "\"HER2 Status\",\"Tumor Other Histologic Subtype\",\"Hormone Therapy\",\"Inferred Menopausal State\",\n",
        "\"Integrative Cluster\",\"Primary Tumor Laterality\",\"Lymph nodes examined positive\",\n",
        "\"Mutation Count\",\"Nottingham prognostic index\",\"Oncotree Code\",\"Overall Survival (Months)\",\n",
        "\"Overall Survival Status\",\"PR Status\",\"Radio Therapy\",\"Relapse Free Status (Months)\",\n",
        "\"Relapse Free Status\",\"Number of Samples Per Patient\",\"Sample Type\",\"Sex\",\n",
        "\"3-Gene classifier subtype\",\"TMB (nonsynonymous)\",\"Tumor Size\",\"Tumor Stage\",\n",
        "\"Patient's Vital Status\"\n",
        "]\n",
        "\n",
        "# Descriptive stats for target\n",
        "print(df[target].value_counts())\n",
        "\n",
        "# =========================================================\n",
        "# 2. Pre-processing for CNN (tabular â†’ numeric matrix)\n",
        "# =========================================================\n",
        "# Filter out classes with only one sample\n",
        "class_counts = df[target].value_counts()\n",
        "classes_to_keep = class_counts[class_counts >= 2].index.tolist()\n",
        "df_filtered = df[df[target].isin(classes_to_keep)].copy()\n",
        "\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "print(f\"Filtered dataset shape: {df_filtered.shape}\")\n",
        "print(f\"Removed {len(df) - len(df_filtered)} samples with rare cancer types\")\n",
        "\n",
        "cat_cols = [c for c in features if df_filtered[c].dtype == \"object\"]\n",
        "num_cols = [c for c in features if c not in cat_cols]\n",
        "\n",
        "pre = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(), num_cols),\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)\n",
        "])\n",
        "\n",
        "X = pre.fit_transform(df_filtered[features])\n",
        "y_classes = sorted(df_filtered[target].unique())\n",
        "y = df_filtered[target].map({c:i for i,c in enumerate(y_classes)}).values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=42)\n",
        "\n",
        "# reshape for 1-D CNN: (batch, channels=1, length)\n",
        "def to_loader(X, y, bs=64):\n",
        "    tens_x = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n",
        "    tens_y = torch.tensor(y, dtype=torch.long)\n",
        "    return DataLoader(TensorDataset(tens_x, tens_y), batch_size=bs, shuffle=True)\n",
        "\n",
        "train_loader = to_loader(X_train, y_train)\n",
        "test_loader  = to_loader(X_test,  y_test, bs=128)\n",
        "\n",
        "# =========================================================\n",
        "# 3. CNN model\n",
        "# =========================================================\n",
        "class Simple1DCNN(nn.Module):\n",
        "    def __init__(self, input_len, n_classes):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64, n_classes)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "cnn = Simple1DCNN(X.shape[1], len(y_classes)).to(device)\n",
        "opt = optim.Adam(cnn.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "cnn_loss_hist = []\n",
        "for epoch in range(10):\n",
        "    cnn.train()\n",
        "    running = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        out = cnn(xb)\n",
        "        loss = loss_fn(out, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running += loss.item()*len(xb)\n",
        "    cnn_loss_hist.append(running/len(train_loader.dataset))\n",
        "    print(f\"Epoch {epoch+1}: loss {cnn_loss_hist[-1]:.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "cnn.eval()\n",
        "preds, ys = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        out = cnn(xb.to(device)).cpu()\n",
        "        preds.extend(out.argmax(1).numpy())\n",
        "        ys.extend(yb.numpy())\n",
        "cnn_acc = accuracy_score(ys, preds)\n",
        "cnn_prec, cnn_rec, cnn_f1, _ = precision_recall_fscore_support(ys, preds, average='macro')\n",
        "\n",
        "# =========================================================\n",
        "# 4. Prepare data for BioBERT\n",
        "# =========================================================\n",
        "def row_to_text(r):\n",
        "    return \" \".join([f\"{col}:{r[col]}\" for col in features])\n",
        "\n",
        "df_filtered[\"text\"] = df_filtered.apply(row_to_text, axis=1)\n",
        "train_df, test_df = train_test_split(df_filtered, test_size=.2, stratify=df_filtered[target], random_state=42)\n",
        "\n",
        "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tok(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True)\n",
        "\n",
        "train_ds = datasets.Dataset.from_pandas(train_df[[\"text\", target]]).rename_column(target, \"label\")\n",
        "test_ds  = datasets.Dataset.from_pandas(test_df[[\"text\", target]]).rename_column(target, \"label\")\n",
        "label2id = {c:i for i,c in enumerate(y_classes)}\n",
        "train_ds = train_ds.map(lambda x: {\"label\": label2id[x[\"label\"]]})\n",
        "test_ds  = test_ds.map(lambda x: {\"label\": label2id[x[\"label\"]]})\n",
        "train_ds = train_ds.map(tok, batched=True, remove_columns=[\"text\"])\n",
        "test_ds  = test_ds.map(tok,  batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(y_classes)).to(device)\n",
        "# Check transformers version to handle API differences\n",
        "import transformers\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "\n",
        "# For older versions of transformers\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./biobert_out\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-5,\n",
        "    report_to='none',  # This is used to disable wandb in case it causes issues\n",
        "    # For older versions of transformers\n",
        "    eval_steps=1,\n",
        "    save_steps=1,\n",
        "    logging_steps=1,\n",
        "    save_total_limit=2\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer)\n",
        ")\n",
        "trainer.train()\n",
        "biobert_loss_hist = trainer.state.log_history  # list of dicts; take train_loss\n",
        "\n",
        "# For older versions of transformers, we need to manually evaluate\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"BioBERT Evaluation Results: {eval_results}\")\n",
        "\n",
        "# Get predictions from the model\n",
        "pred_output = trainer.predict(test_ds)\n",
        "biobert_preds = np.argmax(pred_output.predictions, axis=1)\n",
        "\n",
        "# Calculate metrics manually\n",
        "biobert_acc = accuracy_score(test_ds[\"label\"], biobert_preds)\n",
        "biobert_prec, biobert_rec, biobert_f1, _ = precision_recall_fscore_support(\n",
        "    test_ds[\"label\"],\n",
        "    biobert_preds,\n",
        "    average='macro'\n",
        ")\n",
        "\n",
        "# =========================================================\n",
        "# 5. Compare metrics\n",
        "# =========================================================\n",
        "metrics = pd.DataFrame({\n",
        "    \"Model\":[\"CNN\",\"BioBERT\"],\n",
        "    \"Accuracy\":[cnn_acc, biobert_acc],\n",
        "    \"Precision\":[cnn_prec, biobert_prec],\n",
        "    \"Recall\":[cnn_rec, biobert_rec],\n",
        "    \"F1\":[cnn_f1, biobert_f1]\n",
        "})\n",
        "print(metrics.round(4))\n",
        "\n",
        "# Bar chart\n",
        "metrics_melt = metrics.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(data=metrics_melt, x=\"Metric\", y=\"Score\", hue=\"Model\")\n",
        "plt.ylim(0,1); plt.title(\"CNN vs BioBERT Performance\"); plt.tight_layout(); plt.show()\n",
        "\n",
        "# Loss curves\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(cnn_loss_hist, label=\"CNN\")\n",
        "\n",
        "# Handle different formats of biobert_loss_hist\n",
        "if biobert_loss_hist:\n",
        "    biobert_losses = []\n",
        "    for entry in biobert_loss_hist:\n",
        "        if \"train_loss\" in entry:\n",
        "            biobert_losses.append(entry[\"train_loss\"])\n",
        "    if biobert_losses:\n",
        "        plt.plot(biobert_losses, label=\"BioBERT\")\n",
        "\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training Loss\"); plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()"
      ]
    }
  ]
}